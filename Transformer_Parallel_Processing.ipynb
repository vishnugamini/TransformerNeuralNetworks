{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff769b36-dd73-4864-b596-7d9fdd8f08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff0ce02-baf3-492a-9941-f369a484c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, x: List[int]):\n",
    "        return self.embeddings(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decc72be-f068-4454-8a50-3e5e2f76c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, context_length, embed_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(context_lenth, embed_size)\n",
    "\n",
    "    def forward(self, x: List[int]):\n",
    "        return self.embeddings(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a6500a-e14b-46f8-9f0b-0d7c96f634b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, heads: int, context_length: int, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.qw = nn.Linear(embed_size, embed_size)\n",
    "        self.kw = nn.Linear(embed_size, embed_size)\n",
    "        self.vw = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        assert embed_size % heads == 0, \"embed_size should be completely divisible by heads\"\n",
    "\n",
    "        self.heads = heads\n",
    "        self.head_size = embed_size // heads\n",
    "\n",
    "        self.outproj = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        mask = torch.tril(torch.ones(1,1,context_length, context_length))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "        self.kv_cache = (None, None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.qw(x) # (batch_size, context_len, embed_size)\n",
    "        k = self.kw(x) # (batch_size, context_len, embed_size)\n",
    "        v = self.vw(x) # (batch_size, context_len, embed_size)\n",
    "\n",
    "        batch_size, context_length, embed_size = q.shape\n",
    "\n",
    "        q = q.reshape((batch_size, context_length, self.heads, self.head_size)) # (batch_size, context_len, heads, head_size)\n",
    "        k = k.reshape((batch_size, context_length, self.heads, self.head_size)) # (batch_size, context_len, heads, head_size)\n",
    "        v = v.reshape((batch_size, context_length, self.heads, self.head_size)) # (batch_size, context_len, heads, head_size)\n",
    "\n",
    "        q = q.transpose(1,2) # (batch_size, heads, context_len, head_size)\n",
    "        k = k.transpose(1,2) # (batch_size, heads, context_len, head_size)\n",
    "        v = v.transpose(1,2) # (batch_size, heads, context_len, head_size)\n",
    "\n",
    "        k = k.transpose(-1, -2) # (batch_size, heads, head_size, context_len)\n",
    "\n",
    "        qk = torch.matmul(q,k) # (batch_size, heads, context_len, head_size) @ (batch_size, heads, head_size, context_len) = (batch_size, heads, context_len, context_len)\n",
    "\n",
    "        qk = qk / math.sqrt(self.head_size)\n",
    "\n",
    "        mask = self.mask[:,:,:context_length, :context_length]\n",
    "        \n",
    "        qk = qk.masked_fill(mask == 0, float(\"-inf\")) # (batch_size, heads, context_len, context_len)\n",
    "        \n",
    "        qk = F.softmax(qk, dim = -1)\n",
    "\n",
    "        qk = self.dropout(qk)\n",
    "\n",
    "        qkv = torch.matmul(qk,v) # (batch_size, heads, context_len, context_len) @ (batch_size, heads, context_len, head_size)) = (batch_size, heads, context_len, head_size)\n",
    "\n",
    "        qkv = qkv.transpose(1,2) # (batch_size, context_len, heads, head_size)\n",
    "\n",
    "        qkv = qkv.reshape((batch_size, context_length, -1)) # (batch_size, context_len, heads, head_size)\n",
    "\n",
    "        qkv = self.outproj(qkv)\n",
    "\n",
    "        return qkv\n",
    "\n",
    "    \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafd8766-9ad3-408c-9c99-760dae557099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size: int, heads: int, context_length: int, feed_forward_depth:int, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.block = MultiHeadAttention(embed_size, heads, context_length)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.FFN = nn.Sequential(nn.Linear(embed_size, embed_size * feed_forward_depth),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(embed_size * feed_forward_depth, embed_size),\n",
    "                                 nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.block(self.layernorm1(x))\n",
    "        x = x + self.FFN(self.layernorm2(x))\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
